import argparse
import shutil

import pyrealsense2
from ultralytics import YOLO
import cv2
import numpy as np
from pathlib import Path
import os
import torch

# åŠ¨æ€å¯¼å…¥TensorRTç›¸å…³åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    print("âš ï¸  TensorRTæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡†PyTorchæ¨¡å‹")

# åŠ¨æ€å¯¼å…¥ONNX Runtimeç›¸å…³åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸  ONNX Runtimeæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡†PyTorchæ¨¡å‹")

from utils.calibrationManager import apply_perspective_correction, camera_to_chess_position


class ChessPieceDetectorSeparate():
    def __init__(self, model_path='yolov8s.pt'):
        """
        åˆå§‹åŒ–æ£‹å­æ£€æµ‹å™¨ - çº¢é»‘æ£‹å­åˆ†åˆ«è¯†åˆ«
        æ”¯æŒ.ptã€.trt/.engineå’Œ.onnxæ ¼å¼
        """
        self.model_path = model_path
        self.is_trt_model = model_path.endswith(('.trt', '.engine'))
        self.is_onnx_model = model_path.endswith('.onnx')
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        if self.is_trt_model and TENSORRT_AVAILABLE:
            print("ğŸ”§ åŠ è½½TensorRTä¼˜åŒ–æ¨¡å‹...")
            self.model = self._load_trt_model(model_path)
        elif self.is_onnx_model and ONNX_AVAILABLE:
            print("ğŸ”§ åŠ è½½ONNXæ¨¡å‹...")
            self.model = self._load_onnx_model(model_path)
        else:
            print("ğŸ”§ åŠ è½½æ ‡å‡†YOLOæ¨¡å‹...")
            self.model = YOLO(model_path)
            # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            self.model.to(self.device)

        # çº¢é»‘åŒæ–¹å„7ç§æ£‹å­
        self.class_names = ["A","B", "C", "K", "N", "P", "R", "a", "b", "c", "k", "n", "p", "r"]

        # ä¸ºæ¯ç§æ£‹å­åˆ†é…ä¸åŒé¢œè‰²
        self.colors = [
            # çº¢æ–¹æ£‹å­ - çº¢è‰²ç³»
            (0, 0, 255),    # red_general - çº¯çº¢
            (0, 50, 255),   # red_advisor - æ©™çº¢
            (0, 100, 255),  # red_elephant - æ©™è‰²
            (0, 150, 255),  # red_horse - æ©™é»„
            (0, 200, 255),  # red_chariot - é»„æ©™
            (0, 255, 255),  # red_cannon - é»„è‰²
            (100, 255, 255),# red_soldier - æµ…é»„

            # é»‘æ–¹æ£‹å­ - è“è‰²ç³»/é»‘è‰²ç³»
            (255, 0, 0),    # black_general - è“è‰²
            (255, 50, 0),   # black_advisor - æ·±è“
            (255, 100, 0),  # black_elephant - é›è“
            (255, 150, 0),  # black_horse - ç´«è“
            (255, 200, 0),  # black_chariot - é’è“
            (255, 255, 0),  # black_cannon - é’è‰²
            (128, 128, 0)   # black_soldier - ç°è‰²
        ]


    def _load_trt_model(self, engine_path):
        """
        åŠ è½½TensorRTå¼•æ“æ¨¡å‹
        """
        if not TENSORRT_AVAILABLE:
            raise RuntimeError("TensorRTåº“æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½TensorRTæ¨¡å‹")

        try:
            # åˆ›å»ºTensorRTè¿è¡Œæ—¶
            TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
            with open(engine_path, 'rb') as f:
                runtime = trt.Runtime(TRT_LOGGER)
                self.engine = runtime.deserialize_cuda_engine(f.read())

            # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            self.context = self.engine.create_execution_context()

            # åˆ†é…GPUç¼“å†²åŒº
            self.inputs = []
            self.outputs = []
            self.bindings = []

            for binding in self.engine:
                binding_idx = self.engine.get_binding_index(binding)
                if binding_idx == -1:
                    print(f"âŒ æœªæ‰¾åˆ°ç»‘å®š: {binding}")
                    continue

                # è·å–ç»‘å®šç»´åº¦
                dims = self.engine.get_binding_shape(binding_idx)
                size = trt.volume(dims) * self.engine.max_batch_size * np.dtype(np.float32).itemsize

                # åˆ†é…GPUå†…å­˜
                gpu_mem = cuda.mem_alloc(size)
                self.bindings.append(int(gpu_mem))

                if self.engine.binding_is_input(binding_idx):
                    self.inputs.append({'mem': gpu_mem, 'shape': dims, 'name': binding})
                else:
                    self.outputs.append({'mem': gpu_mem, 'shape': dims, 'name': binding})

            print("âœ… TensorRTæ¨¡å‹åŠ è½½æˆåŠŸ")
            return None  # TRTæ¨¡å‹ä¸éœ€è¦ä¿å­˜ä¸ºself.model
        except Exception as e:
            print(f"âš ï¸  TensorRTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°æ ‡å‡†PyTorchæ¨¡å‹")
            self.is_trt_model = False
            model = YOLO(self.model_path.replace('.trt', '.pt').replace('.engine', '.pt'))
            model.to(self.device)
            return model

    def _load_onnx_model(self, onnx_path):
        """
        åŠ è½½ONNXæ¨¡å‹
        """
        if not ONNX_AVAILABLE:
            raise RuntimeError("ONNX Runtimeæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½ONNXæ¨¡å‹")

        try:
            # æ ¹æ®CUDAå¯ç”¨æ€§é€‰æ‹©æ‰§è¡Œæä¾›è€…
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
            self.onnx_session = ort.InferenceSession(onnx_path, providers=providers)
            print("âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")
            return None
        except Exception as e:
            print(f"âš ï¸  ONNXæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°æ ‡å‡†PyTorchæ¨¡å‹")
            self.is_onnx_model = False
            model = YOLO(onnx_path.replace('.onnx', '.pt'))
            model.to(self.device)
            return model

    def _trt_inference(self, image):
        """
        ä½¿ç”¨TensorRTå¼•æ“æ‰§è¡Œæ¨ç†
        """
        try:
            # é¢„å¤„ç†å›¾åƒ
            input_shape = self.inputs[0]['shape']
            processed_img = self._preprocess_image(image, input_shape)

            # å°†è¾“å…¥æ•°æ®å¤åˆ¶åˆ°GPU
            cuda.memcpy_htod(self.inputs[0]['mem'], processed_img)

            # æ‰§è¡Œæ¨ç†
            self.context.execute_v2(bindings=self.bindings)

            # ä»GPUè·å–è¾“å‡º
            results = []
            for output in self.outputs:
                output_data = np.empty(output['shape'], dtype=np.float32)
                cuda.memcpy_dtoh(output_data, output['mem'])
                results.append(output_data)

            # åå¤„ç†ç»“æœï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è¾“å‡ºæ ¼å¼è¿›è¡Œè°ƒæ•´ï¼‰
            return self._postprocess_results(results, image.shape)

        except Exception as e:
            print(f"âš ï¸  TensorRTæ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†YOLOæ¨ç†
            model = YOLO(self.model_path.replace('.trt', '.pt').replace('.engine', '.pt'))
            model.to(self.device)
            return model(image)

    def _onnx_inference(self, image):
        """
        ä½¿ç”¨ONNXæ¨¡å‹æ‰§è¡Œæ¨ç†
        """
        try:
            # é¢„å¤„ç†å›¾åƒ
            input_name = self.onnx_session.get_inputs()[0].name
            input_shape = self.onnx_session.get_inputs()[0].shape

            # å¦‚æœæ˜¯åŠ¨æ€è¾“å…¥å½¢çŠ¶ï¼Œä½¿ç”¨640ä½œä¸ºé»˜è®¤å°ºå¯¸
            if isinstance(input_shape[2], str) or isinstance(input_shape[3], str):
                input_shape = [input_shape[0], input_shape[1], 640, 640]

            processed_img = self._preprocess_image(image, input_shape)

            # æ‰§è¡Œæ¨ç†
            outputs = self.onnx_session.run(None, {input_name: processed_img.astype(np.float32)})

            # åå¤„ç†ç»“æœ
            return self._postprocess_onnx_results(outputs, image.shape)

        except Exception as e:
            print(f"âš ï¸  ONNXæ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†YOLOæ¨ç†
            model = YOLO(self.model_path.replace('.onnx', '.pt'))
            model.to(self.device)
            return model(image)

    def _preprocess_image(self, image, input_shape):
        """
        é¢„å¤„ç†å›¾åƒä»¥é€‚é…æ¨¡å‹è¾“å…¥
        """
        # è°ƒæ•´å›¾åƒå¤§å°
        resized = cv2.resize(image, (input_shape[2], input_shape[1]))

        # è½¬æ¢é¢œè‰²ç©ºé—´ (BGR to RGB)
        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)

        # å½’ä¸€åŒ–
        normalized = rgb.astype(np.float32) / 255.0

        # è½¬æ¢ä¸ºCHWæ ¼å¼
        chw = np.transpose(normalized, (2, 0, 1))

        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        batched = np.expand_dims(chw, axis=0)

        # è½¬æ¢ä¸ºè¿ç»­å†…å­˜å¸ƒå±€
        return np.ascontiguousarray(batched)

    def _postprocess_results(self, trt_outputs, original_shape):
        """
        åå¤„ç†TensorRTè¾“å‡ºç»“æœ
        æ³¨æ„ï¼šè¿™ä¸ªå®ç°éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è¾“å‡ºæ ¼å¼è¿›è¡Œè°ƒæ•´
        """
        # è¿™é‡Œæ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹å®ç°
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®æ¨¡å‹çš„å…·ä½“è¾“å‡ºæ ¼å¼è¿›è¡Œè§£æ
        # ä¸ºä¿æŒå…¼å®¹æ€§ï¼Œæˆ‘ä»¬æš‚æ—¶å›é€€åˆ°æ ‡å‡†YOLOæ¨¡å‹
        print("âš ï¸  TensorRTåå¤„ç†æœªå®Œå…¨å®ç°ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å‹")
        standard_model = YOLO(self.model_path.replace('.trt', '.pt').replace('.engine', '.pt'))
        standard_model.to(self.device)
        return standard_model(np.zeros(original_shape, dtype=np.uint8))

    def _postprocess_onnx_results(self, onnx_outputs, original_shape):
        """
        åå¤„ç†ONNXè¾“å‡ºç»“æœ
        """
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ONNXæ¨¡å‹è¾“å‡ºæ ¼å¼è¿›è¡Œè°ƒæ•´
        # ä¸ºä¿æŒå…¼å®¹æ€§ï¼Œæš‚æ—¶å›é€€åˆ°æ ‡å‡†YOLOæ¨¡å‹
        print("âš ï¸  ONNXåå¤„ç†æœªå®Œå…¨å®ç°ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å‹")
        standard_model = YOLO(self.model_path.replace('.onnx', '.pt'))
        standard_model.to(self.device)
        return standard_model(np.zeros(original_shape, dtype=np.uint8))

    def convert_to_trt(self, output_path=None, imgsz=640, half=True):
        """
        å°†YOLOæ¨¡å‹è½¬æ¢ä¸ºTensorRTæ ¼å¼

        Args:
            output_path: è¾“å‡ºTensorRTæ¨¡å‹è·¯å¾„
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            half: æ˜¯å¦ä½¿ç”¨FP16ç²¾åº¦
        """
        if not self.model_path.endswith('.pt'):
            print("âŒ  åªæœ‰PyTorchæ¨¡å‹(.pt)å¯ä»¥è½¬æ¢ä¸ºTensorRTæ ¼å¼")
            return None

        if output_path is None:
            model_name = Path(self.model_path).stem
            output_path = f"{model_name}.trt"

        try:
            # ä½¿ç”¨ultralyticsæä¾›çš„å¯¼å‡ºåŠŸèƒ½
            model = YOLO(self.model_path)

            # å¯¼å‡ºä¸ºTensorRTæ ¼å¼
            model.export(
                format='engine',
                imgsz=imgsz,
                half=half,  # ä½¿ç”¨FP16ç²¾åº¦
                device=0 if torch.cuda.is_available() else None,  # ä½¿ç”¨GPU 0æˆ–CPU
                verbose=True
            )

            # é‡å‘½åç”Ÿæˆçš„æ–‡ä»¶
            generated_path = self.model_path.replace('.pt', '.engine')
            if os.path.exists(generated_path):
                shutil.move(generated_path, output_path)

            print(f"âœ… æ¨¡å‹å·²æˆåŠŸè½¬æ¢ä¸ºTensorRTæ ¼å¼: {output_path}")
            return output_path

        except Exception as e:
            print(f"âŒ æ¨¡å‹è½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def convert_to_onnx(self, output_path=None, imgsz=640, dynamic=False):
        """
        å°†YOLOæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼

        Args:
            output_path: è¾“å‡ºONNXæ¨¡å‹è·¯å¾„
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            dynamic: æ˜¯å¦ä½¿ç”¨åŠ¨æ€è¾“å…¥å°ºå¯¸
        """
        if not self.model_path.endswith('.pt'):
            print("âŒ  åªæœ‰PyTorchæ¨¡å‹(.pt)å¯ä»¥è½¬æ¢ä¸ºONNXæ ¼å¼")
            return None

        if output_path is None:
            model_name = Path(self.model_path).stem
            output_path = f"{model_name}.onnx"

        try:
            # ä½¿ç”¨ultralyticsæä¾›çš„å¯¼å‡ºåŠŸèƒ½
            model = YOLO(self.model_path)

            # å¯¼å‡ºä¸ºONNXæ ¼å¼
            model.export(
                format='onnx',
                imgsz=imgsz,
                dynamic=dynamic,  # æ˜¯å¦æ”¯æŒåŠ¨æ€è¾“å…¥å°ºå¯¸
                simplify=True,    # ç®€åŒ–æ¨¡å‹
                device=0 if torch.cuda.is_available() else 'cpu'
            )

            # é‡å‘½åç”Ÿæˆçš„æ–‡ä»¶
            generated_path = self.model_path.replace('.pt', '.onnx')
            if os.path.exists(generated_path):
                os.rename(generated_path, output_path)

            print(f"âœ… æ¨¡å‹å·²æˆåŠŸè½¬æ¢ä¸ºONNXæ ¼å¼: {output_path}")
            return output_path

        except Exception as e:
            print(f"âŒ æ¨¡å‹è½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def train(self, data_yaml='yaml/data.yaml', epochs=300, imgsz=640):
        """
        è®­ç»ƒæ£‹å­æ£€æµ‹æ¨¡å‹
        """
        # ç¡®ä¿ä½¿ç”¨çš„æ˜¯.ptæ¨¡å‹è¿›è¡Œè®­ç»ƒ
        train_model_path = self.model_path
        if self.model_path.endswith(('.trt', '.engine', '.onnx')):
            train_model_path = self._get_pt_model_path()

        model = YOLO(train_model_path)
        # æ ¹æ®CUDAå¯ç”¨æ€§é€‰æ‹©è®¾å¤‡
        device = 0 if torch.cuda.is_available() else 'cpu'
        model.train(
            data=data_yaml,
            epochs=epochs,
            imgsz=imgsz,
            batch=16,
            device=device,
            name='chess_piece_detection_separate'
        )

    def _get_pt_model_path(self):
        """
        è·å–å¯¹åº”çš„.ptæ¨¡å‹è·¯å¾„
        """
        if self.model_path.endswith(('.trt', '.engine')):
            return self.model_path.replace('.trt', '.pt').replace('.engine', '.pt')
        elif self.model_path.endswith('.onnx'):
            return self.model_path.replace('.onnx', '.pt')
        return self.model_path

    def _run_inference(self, image, conf_threshold=0.5, iou_threshold=0.25):
        """
        æ ¹æ®æ¨¡å‹ç±»å‹æ‰§è¡Œæ¨ç†
        """
        if self.is_trt_model and TENSORRT_AVAILABLE:
            return self._trt_inference(image)
        elif self.is_onnx_model and ONNX_AVAILABLE:
            return self._onnx_inference(image)
        else:
            return self.model(image, conf=conf_threshold, iou=iou_threshold)

    def detect(self, image_path, conf_threshold=0.5, iou_threshold=0.8, save_path='result.jpg'):
        """
        æ£€æµ‹å›¾åƒä¸­çš„æ£‹å­å¹¶ä¿å­˜ç»“æœå›¾ç‰‡
        :param image_path: è¾“å…¥å›¾åƒè·¯å¾„
        :param conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        :param iou_threshold: IOU é˜ˆå€¼
        :param save_path: ä¿å­˜ç»“æœå›¾åƒçš„è·¯å¾„
        """
        # è¯»å–å›¾åƒ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")

        # æ‰§è¡Œæ£€æµ‹
        results = self._run_inference(image, conf_threshold, iou_threshold)

        # å¯è§†åŒ–æ£€æµ‹ç»“æœ
        vis_image = self.visualize_detections(image, results)

        # ä¿å­˜ç»“æœå›¾åƒ
        cv2.imwrite(save_path, vis_image)
        print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_path}")

    def visualize_detections(self, image, results):
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœï¼ˆå¢åŠ å»é‡é€»è¾‘ï¼‰
        """
        # å¤åˆ¶å›¾åƒç”¨äºç»˜åˆ¶
        img_vis = image.copy()

        # è·å–æ£€æµ‹ç»“æœ
        boxes = None
        if hasattr(results, '__len__') and len(results) > 0:
            boxes = results[0].boxes if hasattr(results[0], 'boxes') else None
        else:
            boxes = results[0].boxes if hasattr(results, '__len__') and hasattr(results[0], 'boxes') else None

        if boxes is not None and len(boxes) > 0:
            # æ·»åŠ å»é‡é€»è¾‘ï¼šå¯¹ç›¸åŒç±»åˆ«çš„é‡å æ¡†è¿›è¡Œèšç±»
            filtered_boxes = self._filter_duplicate_detections(boxes)

            # éå†æ¯ä¸ªæ£€æµ‹åˆ°çš„æ£‹å­
            for box in filtered_boxes:
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                conf = float(box.conf[0].cpu().numpy())
                cls = int(box.cls[0].cpu().numpy())

                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                color = self.colors[cls]
                cv2.rectangle(img_vis, (x1, y1), (x2, y2), color, 2)

                # ç»˜åˆ¶æ ‡ç­¾
                label = f'{self.class_names[cls]} {conf:.2f}'
                cv2.putText(img_vis, label, (x1, y1-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        return img_vis

    def _filter_duplicate_detections(self, boxes, iou_threshold=0.5):
        """
        è¿‡æ»¤é‡å¤æ£€æµ‹æ¡†
        """
        if len(boxes) <= 1:
            return boxes

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        confidences = [float(box.conf[0].cpu().numpy()) for box in boxes]
        sorted_indices = sorted(range(len(confidences)), key=lambda i: confidences[i], reverse=True)

        keep_boxes = []
        used_indices = set()

        for i in sorted_indices:
            if i in used_indices:
                continue

            current_box = boxes[i]
            keep_boxes.append(current_box)
            used_indices.add(i)

            # è®¡ç®—ä¸å½“å‰æ¡†çš„IOUï¼Œæ ‡è®°é‡å çš„ä½ç½®ä¿¡åº¦æ¡†
            for j in sorted_indices:
                if j in used_indices:
                    continue

                iou = self._calculate_iou(current_box.xyxy[0].cpu().numpy(),
                                        boxes[j].xyxy[0].cpu().numpy())
                if iou > iou_threshold:
                    used_indices.add(j)

        return keep_boxes

    def _calculate_iou(self, box1, box2):
        """
        è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IOU
        """
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])

        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0

    def save_image_with_keypoints(self, image_path, output_path, half_board="red", conf_threshold=0.5, iou_threshold=0.4):
        """
        æ£€æµ‹å›¾åƒä¸­çš„å…³é”®ç‚¹å¹¶ä¿å­˜å¸¦å…³é”®ç‚¹çš„å›¾åƒ

        :param image_path: è¾“å…¥å›¾åƒè·¯å¾„
        :param output_path: è¾“å‡ºå›¾åƒä¿å­˜è·¯å¾„
        :param half_board: åŠåŒºç±»å‹ ("red" æˆ– "black")
        :param conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        :param iou_threshold: IOUé˜ˆå€¼
        """
        # è¯»å–å›¾åƒ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")

        # å¤åˆ¶å›¾åƒç”¨äºç»˜åˆ¶
        img_vis = image.copy()

        try:
            # å¯¼å…¥å‚æ•°æ–‡ä»¶ä¸­çš„å›ºå®šåæ ‡
            from parameters import CHESS_BOARD_R, CHESS_BOARD_B

            # æ ¹æ®half_boardå‚æ•°é€‰æ‹©ä½¿ç”¨å“ªä¸€ç»„å›ºå®šåæ ‡
            if half_board == "red":
                # ä½¿ç”¨çº¢æ–¹åŠç›˜çš„å››ä¸ªè§’ç‚¹åæ ‡
                fixed_points = CHESS_BOARD_R
            else:  # black
                # ä½¿ç”¨é»‘æ–¹åŠç›˜çš„å››ä¸ªè§’ç‚¹åæ ‡
                fixed_points = CHESS_BOARD_B

            # ç»˜åˆ¶å›ºå®šçš„å…³é”®ç‚¹
            for i, (x, y) in enumerate(fixed_points):
                # ç»˜åˆ¶å…³é”®ç‚¹ï¼ˆä½¿ç”¨ä¸åŒé¢œè‰²åŒºåˆ†ï¼‰
                cv2.circle(img_vis, (int(x), int(y)), 8, (0, 255, 0), -1)  # ç»¿è‰²åœ†ç‚¹
                # æ·»åŠ å…³é”®ç‚¹ç¼–å·
                cv2.putText(img_vis, str(i), (int(x)+10, int(y)+10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

            # ç»˜åˆ¶æ£‹ç›˜è¾¹ç•Œæ¡†
            pts = np.array(fixed_points, np.int32)
            pts = pts.reshape((-1, 1, 2))
            cv2.polylines(img_vis, [pts], True, (255, 0, 0), 2)

        except ImportError:
            print("âš ï¸  æ— æ³•å¯¼å…¥å‚æ•°æ–‡ä»¶ï¼Œè·³è¿‡å›ºå®šå…³é”®ç‚¹ç»˜åˆ¶")

        # ä¿å­˜ç»“æœå›¾åƒ
        cv2.imwrite(output_path, img_vis)
        print(f"âœ… å¸¦å…³é”®ç‚¹çš„å›¾åƒå·²ä¿å­˜è‡³: {output_path}")
    def extract_chessboard_layout_with_height(self, image, chess_points, half_board="red",
                                              conf_threshold=0.5, iou_threshold=0.4):
        """
        ä»å›¾åƒä¸­æå–æ£‹ç›˜å¸ƒå±€

        :param image: è¾“å…¥å›¾åƒ
        :param half_board: åŠåŒºç±»å‹ ("red" æˆ– "black")
        :param conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        :param iou_threshold: IOUé˜ˆå€¼
        :return: æ£‹ç›˜å¸ƒå±€çŸ©é˜µå’Œæ£€æµ‹ç»“æœ
        """
        if image is None:
            # å¦‚æœæ²¡æœ‰å›¾åƒï¼Œè¿”å›ç©ºæ£‹ç›˜
            empty_layout = [['.' for _ in range(9)] for _ in range(5)]
            return empty_layout, None, {}

        # æ‰§è¡Œæ£€æµ‹
        results = self._run_inference(image, conf_threshold, iou_threshold)

        # åˆå§‹åŒ–5x9æ£‹ç›˜ï¼ˆåŠä¸ªæ£‹ç›˜ï¼‰
        chess_layout = [['.' for _ in range(9)] for _ in range(5)]
        points_center = {}
        # å­˜å‚¨æ£‹å­é«˜åº¦ä¿¡æ¯
        piece_heights = {}

        # è·å–æ£€æµ‹ç»“æœ
        boxes = results[0].boxes if hasattr(results, '__len__') and hasattr(results[0], 'boxes') else None

        if boxes is not None and len(boxes) > 0:
            # æå–æ‰€æœ‰æ£€æµ‹æ¡†ä¿¡æ¯ç”¨äºå»é‡
            detections = []
            for box in boxes:
                # è·å–è¾¹ç•Œæ¡†åæ ‡å’Œç±»åˆ«
                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                conf = float(box.conf[0].cpu().numpy())
                cls = int(box.cls[0].cpu().numpy())

                # è®¡ç®—æ£‹å­ä¸­å¿ƒç‚¹
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2

                # å°†å›¾åƒåæ ‡è½¬æ¢ä¸ºæ£‹ç›˜åæ ‡
                if half_board == 'red':
                    chess_pos = camera_to_chess_position(center_x, center_y, chess_points)
                else:
                    chess_pos = camera_to_chess_position(center_x, center_y, chess_points)

                if chess_pos is not None:
                    detections.append({
                        'box': box,
                        'chess_pos': chess_pos,
                        'conf': conf,
                        'cls': cls,
                        'center': (center_x, center_y),
                    })

            # æŒ‰ç…§æ£‹ç›˜ä½ç½®å¯¹æ£€æµ‹ç»“æœè¿›è¡Œåˆ†ç»„
            position_detections = {}
            for detection in detections:
                row, col = detection['chess_pos']
                position_key = (row, col)

                if position_key not in position_detections:
                    position_detections[position_key] = []
                position_detections[position_key].append(detection)

            # å¯¹æ¯ä¸ªä½ç½®çš„å¤šä¸ªæ£€æµ‹ç»“æœè¿›è¡Œå»é‡ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
            filtered_detections = []
            for position_key, detections_list in position_detections.items():
                # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œä¿ç•™æœ€é«˜çš„
                best_detection = max(detections_list, key=lambda x: x['conf'])
                filtered_detections.append(best_detection)

            # éå†å»é‡åçš„æ£€æµ‹ç»“æœ
            for detection in filtered_detections:
                row, col = detection['chess_pos']
                cls = detection['cls']
                # å°†æ£€æµ‹åˆ°çš„æ£‹å­æ”¾å…¥æ£‹ç›˜å¸ƒå±€
                chess_layout[row][col] = self.class_names[cls]
                if half_board == 'red':
                    points_center[f"{9-row}{8-col}"] = detection['center']
                else:
                    points_center[f"{row}{col}"] = detection['center']

        return chess_layout, results,points_center

    def detect_objects_with_height(self, image, depth_frame=None, conf_threshold=0.5, iou_threshold=0.4, mat=None):
        """
        æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“å¹¶è·å–å…¶ä½ç½®å’Œé«˜åº¦ä¿¡æ¯ï¼Œä¸æ¶‰åŠæ£‹ç›˜é€»è¾‘

        :param image: è¾“å…¥å›¾åƒ
        :param depth_frame: æ·±åº¦å¸§æ•°æ®ï¼ˆå¯é€‰ï¼‰
        :param conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        :param iou_threshold: IOUé˜ˆå€¼
        :return: ç‰©ä½“æ£€æµ‹ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«ç±»åˆ«ã€è¾¹ç•Œæ¡†åæ ‡å’Œé«˜åº¦ä¿¡æ¯
        """
        if image is None:
            return []

        # æ‰§è¡Œæ£€æµ‹
        results = self._run_inference(image, conf_threshold, iou_threshold)

        # å­˜å‚¨æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯
        objects_info = []

        # è·å–æ£€æµ‹ç»“æœ
        boxes = results[0].boxes if hasattr(results, '__len__') and hasattr(results[0], 'boxes') else None

        if boxes is not None and len(boxes) > 0:
            # éå†æ¯ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“
            detections = []
            for box in boxes:
                # è·å–è¾¹ç•Œæ¡†åæ ‡å’Œç±»åˆ«
                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                conf = float(box.conf[0].cpu().numpy())
                cls = int(box.cls[0].cpu().numpy())

                # è®¡ç®—ç‰©ä½“ä¸­å¿ƒç‚¹
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2

                # åˆ›å»ºç‰©ä½“ä¿¡æ¯å­—å…¸
                detection = {
                    'box': box,
                    'class_id': cls,
                    'class_name': self.class_names[cls] if cls < len(self.class_names) else f"unknown_{cls}",
                    'bbox': (x1, y1, x2, y2),
                    'center': (center_x, center_y),
                    'confidence': conf
                }

                # è·å–é«˜åº¦ä¿¡æ¯ï¼ˆå¦‚æœæä¾›äº†æ·±åº¦å¸§ï¼‰
                if depth_frame is not None:
                    try:
                        # è·å–è¯¥ç‚¹çš„æ·±åº¦å€¼
                        if mat is not None:
                            x, y = apply_perspective_correction(mat, center_x, center_y)
                        else:
                            x, y = center_x, center_y
                        depth_value = depth_frame.get_distance(x, y)
                        if depth_value == 0:
                            depth_value = depth_frame.get_distance(x+5, y+5)
                        depth_intrinsics = depth_frame.profile.as_video_stream_profile().intrinsics
                        camera_xyz = pyrealsense2.rs2_deproject_pixel_to_point(
                            depth_intrinsics,
                            [float(center_x), float(center_y)],
                            depth_value
                        )
                        camera_xyz = np.round(np.array(camera_xyz), 3)
                        # detection['center'] = (camera_xyz[0], camera_xyz[1])
                        detection['height'] = camera_xyz[2]
                    except Exception as e:
                        print(f"Error processing depth information: {e}")
                        detection['height'] = None
                else:
                    detection['height'] = None

                detections.append(detection)

            # æŒ‰ç½®ä¿¡åº¦æ’åº
            detections.sort(key=lambda x: x['confidence'], reverse=True)

            # å»é‡é€»è¾‘ï¼šä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
            filtered_detections = []
            used_boxes = []

            for detection in detections:
                current_box = detection['bbox']
                is_overlap = False

                # æ£€æŸ¥å½“å‰æ£€æµ‹æ¡†æ˜¯å¦ä¸å·²ä¿ç•™çš„æ£€æµ‹æ¡†é‡å 
                for used_box in used_boxes:
                    iou = self._calculate_iou(
                        [current_box[0], current_box[1], current_box[2], current_box[3]],
                        [used_box[0], used_box[1], used_box[2], used_box[3]]
                    )
                    if iou > iou_threshold:
                        is_overlap = True
                        break

                # å¦‚æœä¸é‡å ï¼Œåˆ™ä¿ç•™è¯¥æ£€æµ‹ç»“æœ
                if not is_overlap:
                    filtered_detections.append(detection)
                    used_boxes.append(current_box)

            objects_info = filtered_detections

        return objects_info



# åœ¨ä¸»ç¨‹åºä¸­ä½¿ç”¨
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str,
                        default='runs/detect/chess_piece_detection_separate7/weights/best.pt',
                        help='æ¨¡å‹è·¯å¾„ (.pt æˆ– .trt/.engine æˆ– .onnx)')
    parser.add_argument('--convert_to', default='onnx', action='store_true',
                        help='å°†.ptæ¨¡å‹è½¬æ¢ä¸ºTensorRT/onnxæ ¼å¼')
    parser.add_argument('--imgsz', type=int, default=640,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--conf_threshold', type=float, default=0.45,
                        help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou_threshold', type=float, default=0.25,
                        help='IOUé˜ˆå€¼')

    args = parser.parse_args()

    # åˆ›å»ºæ£€æµ‹å™¨å®ä¾‹
    detector = ChessPieceDetectorSeparate()
    detector.train()

    # # å¦‚æœéœ€è¦è½¬æ¢æ¨¡å‹ä¸ºTensorRT
    # if args.convert_to=='trt' and args.model_path.endswith('.pt'):
    #     trt_model_path = detector.convert_to_trt(imgsz=args.imgsz)
    #     if trt_model_path:
    #         print(f"âœ… æ¨¡å‹è½¬æ¢æˆåŠŸ: {trt_model_path}")
    #         detector = ChessPieceDetectorSeparate(trt_model_path)
    #     else:
    #         print("âŒ æ¨¡å‹è½¬æ¢å¤±è´¥")
    #
    # # å¦‚æœéœ€è¦è½¬æ¢æ¨¡å‹ä¸ºONNX
    # elif args.convert_to=='onnx' and args.model_path.endswith('.pt'):
    #     onnx_model_path = detector.convert_to_onnx(imgsz=args.imgsz)
    #     if onnx_model_path:
    #         print(f"âœ… æ¨¡å‹è½¬æ¢æˆåŠŸ: {onnx_model_path}")
    #         detector = ChessPieceDetectorSeparate(onnx_model_path)
    #     else:
    #         print("âŒ æ¨¡å‹è½¬æ¢å¤±è´¥")
    #
    # # æ‰§è¡Œæ£€æµ‹
    # detector.detect(
    #     "data/images/val/RS_20250730_111927.jpg",
    #     conf_threshold=args.conf_threshold,
    #     iou_threshold=args.iou_threshold,
    #     save_path="result_with_keypoints.jpg"
    # )
